{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Tags\n",
    "from bs4 import BeautifulSoup\n",
    "soup_text=[]\n",
    "for text in df['review']:\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    soup_text.append(soup.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "token_texts=[]\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "for text in soup_text:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_texts.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop-words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "no_stopword_text=[]\n",
    "for text in token_texts:\n",
    "    no_stopword = [w for w in text if not w in stop_words]\n",
    "    no_stopword_text.append(no_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations, numbers, special characters\n",
    "no_punct_text=[]\n",
    "for words in no_stopword_text:\n",
    "     no_punct_text.append([word for word in words if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "lemm_text=[]\n",
    "for text in no_punct_text:\n",
    "    lemm_text.append(\" \".join(stemmer.lemmatize(sent) for sent in text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text=[]\n",
    "for sent in lemm_text:\n",
    "    final_text.append(sent.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i thought wonderful way spend time hot summer weekend sitting air conditioned theater watching the plot simplistic dialogue witty character likable even well bread suspected serial killer while may disappointed realize match point risk addiction i thought proof woody allen still fully control style many u grown i laughed one woody comedy year dare i say decade while i never impressed scarlet johanson managed tone sexy image jumped right average spirited young may crown jewel career wittier devil wears prada interesting superman great comedy go see friend'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_text), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "all_text2 = ' '.join(final_text)\n",
    "# create a list of words\n",
    "words = all_text2.split()\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {w:i for i, (w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_int = []\n",
    "for review in final_text:\n",
    "    r = [vocab_to_int[w] for w in review.split()]\n",
    "    reviews_int.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    50000.000000\n",
       "mean       116.131160\n",
       "std         86.389722\n",
       "min          4.000000\n",
       "25%         63.000000\n",
       "50%         87.000000\n",
       "75%        141.000000\n",
       "max       1357.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "reviews_len = [len(x) for x in reviews_int]\n",
    "pd.Series(reviews_len).hist()\n",
    "plt.show()\n",
    "pd.Series(reviews_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_int = [ reviews_int[i] for i, l in enumerate(reviews_len) if l>0 ]\n",
    "encoded_labels = [ Y[i] for i, l in enumerate(reviews_len) if l> 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_int, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=pad_features(reviews_int=reviews_int,seq_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_feat=len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "train_x = features[0:int(split_frac*len_feat)]\n",
    "train_y = encoded_labels[0:int(split_frac*len_feat)]\n",
    "remaining_x = features[int(split_frac*len_feat):]\n",
    "remaining_y = encoded_labels[int(split_frac*len_feat):]\n",
    "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
    "valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
    "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
    "test_y = remaining_y[int(len(remaining_y)*0.5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(np.asarray(train_x)), torch.from_numpy(np.asarray(train_y)))\n",
    "valid_data = TensorDataset(torch.from_numpy(np.asarray(valid_x)), torch.from_numpy(np.asarray(valid_y)))\n",
    "test_data = TensorDataset(torch.from_numpy(np.asarray(test_x)), torch.from_numpy(np.asarray(test_y)))\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,  ...,  5096, 25172,    29],\n",
      "        [ 1557,     1,   318,  ...,  1265,    15,     3],\n",
      "        [    0,     0,     0,  ...,    50,  1153,   138],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,    62,    72,     2],\n",
      "        [    0,     0,     0,  ...,   666, 31108,   509],\n",
      "        [    0,     0,     0,  ...,    53,  1316,    96]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(90649, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Loss: 0.818803... Val Loss: 0.581551\n",
      "Epoch: 1/4... Step: 200... Loss: 0.660478... Val Loss: 0.677966\n",
      "Epoch: 1/4... Step: 300... Loss: 0.473516... Val Loss: 0.522080\n",
      "Epoch: 1/4... Step: 400... Loss: 0.370340... Val Loss: 0.509130\n",
      "Epoch: 1/4... Step: 500... Loss: 0.391925... Val Loss: 0.446131\n",
      "Epoch: 1/4... Step: 600... Loss: 0.424984... Val Loss: 0.457974\n",
      "Epoch: 1/4... Step: 700... Loss: 0.496538... Val Loss: 0.430185\n",
      "Epoch: 1/4... Step: 800... Loss: 0.387720... Val Loss: 0.390607\n",
      "Epoch: 2/4... Step: 900... Loss: 0.266648... Val Loss: 0.365274\n",
      "Epoch: 2/4... Step: 1000... Loss: 0.473791... Val Loss: 0.368675\n",
      "Epoch: 2/4... Step: 1100... Loss: 0.330498... Val Loss: 0.361231\n",
      "Epoch: 2/4... Step: 1200... Loss: 0.293538... Val Loss: 0.352406\n",
      "Epoch: 2/4... Step: 1300... Loss: 0.338832... Val Loss: 0.333668\n",
      "Epoch: 2/4... Step: 1400... Loss: 0.220467... Val Loss: 0.349148\n",
      "Epoch: 2/4... Step: 1500... Loss: 0.264746... Val Loss: 0.329365\n",
      "Epoch: 2/4... Step: 1600... Loss: 0.301645... Val Loss: 0.318226\n",
      "Epoch: 3/4... Step: 1700... Loss: 0.177116... Val Loss: 0.354299\n",
      "Epoch: 3/4... Step: 1800... Loss: 0.280256... Val Loss: 0.366411\n",
      "Epoch: 3/4... Step: 1900... Loss: 0.301532... Val Loss: 0.336677\n",
      "Epoch: 3/4... Step: 2000... Loss: 0.193507... Val Loss: 0.336229\n",
      "Epoch: 3/4... Step: 2100... Loss: 0.195196... Val Loss: 0.347185\n",
      "Epoch: 3/4... Step: 2200... Loss: 0.168695... Val Loss: 0.332277\n",
      "Epoch: 3/4... Step: 2300... Loss: 0.208525... Val Loss: 0.320412\n",
      "Epoch: 3/4... Step: 2400... Loss: 0.394609... Val Loss: 0.335425\n",
      "Epoch: 4/4... Step: 2500... Loss: 0.159265... Val Loss: 0.398879\n",
      "Epoch: 4/4... Step: 2600... Loss: 0.399280... Val Loss: 0.421346\n",
      "Epoch: 4/4... Step: 2700... Loss: 0.182940... Val Loss: 0.415366\n",
      "Epoch: 4/4... Step: 2800... Loss: 0.096023... Val Loss: 0.403645\n",
      "Epoch: 4/4... Step: 2900... Loss: 0.032697... Val Loss: 0.387296\n",
      "Epoch: 4/4... Step: 3000... Loss: 0.158789... Val Loss: 0.357578\n",
      "Epoch: 4/4... Step: 3100... Loss: 0.029252... Val Loss: 0.400742\n",
      "Epoch: 4/4... Step: 3200... Loss: 0.063914... Val Loss: 0.377737\n"
     ]
    }
   ],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                inputs = inputs.type(torch.LongTensor)\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.406\n",
      "Test accuracy: 0.859\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    inputs = inputs.type(torch.LongTensor)\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type SentimentLSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net, 'entire_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_review(test_review):\n",
    "    #############\n",
    "    test_review=BeautifulSoup(test_review, 'html.parser').getText()\n",
    "    test_review=tokenizer.tokenize(test_review)\n",
    "    test_review=[w for w in test_review if not w in stop_words]\n",
    "    test_review=[word for word in test_review if word.isalpha()]\n",
    "    test_review=\" \".join(stemmer.lemmatize(sent) for sent in test_review)\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    ############\n",
    "    \n",
    "    # get rid of punctuation\n",
    "    #test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_review.split(\" \")\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "\n",
    "\n",
    "def predict(net, test_review, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "   # print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_new = torch.load('entire_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your review here:  the movie was bad. Did not like it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative review detected.\n"
     ]
    }
   ],
   "source": [
    "seq_length=200      \n",
    "test_review_neg=input('Enter your review here: ')     \n",
    "predict(net, test_review_neg, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
